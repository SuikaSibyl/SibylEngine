#version 450
#extension GL_KHR_shader_subgroup_basic: require

#define ELEMENT_NUM 2048
#define BLOCK_SIZE 1024
#define WARP_PER_BLOCK BLOCK_SIZE/32
#define TILE_SIZE 2048
#define TILE_PER_BLOCK 1
#define ELEMENT_PER_BLOCK TILE_SIZE*TILE_PER_BLOCK
#define ACTIVE_BLOCK_NUM ((ELEMENT_NUM+ELEMENT_PER_BLOCK - 1)/ELEMENT_PER_BLOCK)
#define DIGITS 8
#define POSSIBLE_DIGIT_VALUE 2

layout (local_size_x = 1024) in;

layout(push_constant) uniform constants {
    uint digitOffset;
} PushConstants;

// SIZE = ELEMENT_COUNT
layout(set = 0, binding = 0, std430) buffer InputKeys
{
    uint keys[];
};

layout(set = 0, binding = 1, std430) buffer SortedIndex
{
    uint indices[];
};

// OffsetFromDigitStarts are global using decoupled look-back
// It is a 2-D buffer
// Here is an example for 4-way 16-bits parallel radix sorting
//           x-axis = tiles:
// y-axis =         0   1   2   3   4  ...  t
// possible     0  [0] [1] [2] [3] [4] ... [t]
// values       1  [.] [.] [.] [.] [.] ... [.]
// (bins)       2  [.] [.] [.] [.] [.] ... [.]
//              3  [.] [.] [.] [.] [.] ... [.]
layout(set = 0, binding = 2, std430) buffer OffsetFromDigitStartsAggregate
{
    uint aggregate[];
};
layout(set = 0, binding = 3, std430) buffer OffsetFromDigitStartsPrefix
{
    uint prefix[];
};

// // Global Histograms are scanned for the whole-array.
// // It is a 2-D buffer
// // Here is an example for 4-way 16-bits parallel radix sorting
// //           x-axis = possible value:
// // y-axis =         0   1   2   3
// // digit offset 0  [0] [1] [2] [3]
// //              1  [4] [5] [6] [7]
// //              2  [8] [.] [.] [.]
// //              3  [.] [.] [.] [.]
// //              .   .   .   .   .
// //              7  [.] [.] [.] [.]
layout(set = 0, binding = 4, std430) buffer GlobalDigitHistogram
{
    uint gGlobalHistogram[];
};
uint getGlobalDigitHistogram(uint pass, uint digit_value)
{
    return gGlobalHistogram[pass*POSSIBLE_DIGIT_VALUE + digit_value];
}

const uint already_prepared = 0x80000000;
const uint low_pass_mask = 0x7FFFFFFF;

shared uint shared_match_masks[WARP_PER_BLOCK][DIGITS]; // init with 0

// Use a local prefix sum to get aggregate value for this tile
// and also local prefix sum
void countAggregateValue()
{
    uint thid = gl_LocalInvocationID.x;
    uint warp_idx = gl_SubgroupID;

    // 1. warp digit prefix
    // - 1.1 warp-private digit histogram
    //atomicOr(shared_match_masks[warp_idx][digit], 1 << lane);
    // - 1.2 incremented by leader
    // - 1.3 add to rank
    subgroupBarrier();
    // 2. within thread block
    // - 2.1 exclusive prefix sum of warp digit count
    // - 2.2 add to rank
}

shared uint temp[2*BLOCK_SIZE];

void countAggregateValue_NAIVE()
{

}

// Use the decoupled look-back to get the offset from digit starts
// 
void getInclusivePrefix()
{

}

// For each pass: N reads & N writes
// It is written for 32bits keys & 8bits digits
// We will need 4 such passes to shuffle the whole array.
void oneSweepPass()
{
    countAggregateValue();
    getInclusivePrefix();
}

void main()
{
    uint thid = gl_LocalInvocationID.x;
    uint wgid = gl_WorkGroupID.x;

    uint mask = 0x00000001 << PushConstants.digitOffset;
    uint in_array = 0;
    uint out_array = 1;
    // load keys into shared memory
    // each thread handle 2 keys
    uint passMask_a = keys[indices[wgid*TILE_SIZE*TILE_PER_BLOCK + 2*thid + 0 + ELEMENT_NUM*in_array]] & mask;
    uint passMask_b = keys[indices[wgid*TILE_SIZE*TILE_PER_BLOCK + 2*thid + 1 + ELEMENT_NUM*in_array]] & mask;
    temp[2*thid] = passMask_a;
    temp[2*thid+1] = passMask_b;

    if (thid == 0)
    {
        aggregate[wgid] = 0;
        prefix[wgid] = 0;
    }
    memoryBarrier();

    // build sum in place up the tree 
    uint offset = 1;
    for (int d = TILE_SIZE>>1; d>0; d>>=1)
    {
        barrier();
        if(thid < d)
        {
            uint ai = offset*(2*thid+1)-1;
            uint bi = offset*(2*thid+2)-1;  
            temp[bi] += temp[ai];
        }
        offset *= 2;
    }

    barrier();
    uint workgroupAggregate = temp[TILE_SIZE - 1];
    
    if (thid == 0)
    {
        aggregate[wgid] = workgroupAggregate;
        memoryBarrier();
        // set prepared
        aggregate[wgid] |= already_prepared; 
        // clear the last element
        uint exclusive_prefix = 0;
        for(int i = int(wgid)-1; i >= 0; i--)
        {
            if((prefix[i] & already_prepared) != 0)
            {
                exclusive_prefix += prefix[i] & low_pass_mask;
                break;
            }
            else
            {
                // stall until aggregate is prepared
                while((aggregate[i] & already_prepared)==0){}
                exclusive_prefix += aggregate[i] & low_pass_mask;
            }
        }
        prefix[wgid] = exclusive_prefix + temp[TILE_SIZE - 1];
        memoryBarrier();
        // set prepared
        prefix[wgid] |= already_prepared;

        temp[TILE_SIZE - 1] = 0;
    }

    for (int d = 1; d < TILE_SIZE; d *= 2) // traverse down tree & build scan
    {
        offset >>= 1;
        barrier();
        if (thid < d)
        {
            uint ai = offset*(2*thid+1)-1;
            uint bi = offset*(2*thid+2)-1; 
            uint t = temp[ai];
            temp[ai] = temp[bi];
            temp[bi] += t & low_pass_mask;
        }
    }
    uint exclusive_prefix = (prefix[wgid]&low_pass_mask) - workgroupAggregate;
    uint ai = 2*thid;
    uint bi = 2*thid + 1;

    uint target_pos_ai = (temp[ai]&low_pass_mask) + exclusive_prefix + getGlobalDigitHistogram(0, passMask_a);
    uint target_pos_bi = (temp[bi]&low_pass_mask) + exclusive_prefix + getGlobalDigitHistogram(0, passMask_b);

    indices[target_pos_ai + ELEMENT_NUM*out_array] = indices[ai + ELEMENT_NUM*in_array];
    indices[target_pos_bi + ELEMENT_NUM*out_array] = indices[ai + ELEMENT_NUM*in_array];
}